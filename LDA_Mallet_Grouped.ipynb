{"cells":[{"cell_type":"markdown","source":["LDA : Thanks to machinelearningplus.com for their educational guide on LDA \n","https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"],"metadata":{"id":"sesitjADiw0h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJqrmUXvna0u"},"outputs":[],"source":["!pip install pyLDAvis # install then restart runtime "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mT9_r5voxOF"},"outputs":[],"source":["!pip install -U spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8K3E239G90g0"},"outputs":[],"source":["!pip install --upgrade gensim==3.8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_ZE9PwgtBWM"},"outputs":[],"source":["#install Mallet\n","!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n","!unzip mallet-2.0.8.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFx7pn2g8db_"},"outputs":[],"source":["#to be run only once \n","import os       #importing os to set environment variable\n","def install_java():\n","  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n","  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n","  !java -version       #check java version\n","install_java()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGXjFL4H-DsG"},"outputs":[],"source":["!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGN1jrCPi_V-"},"outputs":[],"source":["## Main libraries \n","import pandas as pd\n","import numpy as np\n","\n","# Preprocessing\n","import gensim\n","from gensim.utils import simple_preprocess \n","from gensim.parsing.preprocessing import STOPWORDS\n","import re\n","from gensim.models.wrappers import LdaMallet\n","\n","# to upload local files to Google cloud #in case used \n","from google.colab import files\n","\n","# Lemmatization\n","import spacy\n","\n","import gensim.corpora as corpora\n","\n","# Plotting tools\n","import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis \n","#import pyLDAvis.gensim as gensimvis # with older versions of pyLDAvis\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_dQO01r4qpcb"},"source":["Load extracted submissions and comments "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMEXwD-LiJjU"},"outputs":[],"source":["# use this as many Comments files you have, just change the dataframe number and file name \n","cdf= pd.read_csv('/content/drive/MyDrive/TMData/comments_ve2y.csv', on_bad_lines='skip') \n","cdf1= pd.read_csv('/content/drive/MyDrive/TMData/comments_ca1y.csv', on_bad_lines='skip') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dwn_DxRaih_t"},"outputs":[],"source":["cdf_ve = cdf[cdf['created_utc']> 1606780799].copy() # selecting comments after 1/12/2020 [as I have some older data in my file]\n","print(cdf_ve.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ru-haypki8fu"},"outputs":[],"source":["# Combine all Comments dataframes in one dataframe \n","cdf_master = cdf_ve.append([cdf1], ignore_index= True)  #ignore_index= True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QRgW3OGjtT5"},"outputs":[],"source":["# use this as many Submissions files you have, just change the dataframe number and file name \n","sdf= pd.read_csv('/content/drive/MyDrive/TMData/submissions_ve1y.csv', on_bad_lines='skip'); \n","sdf1= pd.read_csv('/content/drive/MyDrive/TMData/submissions_ca1y.csv', on_bad_lines='skip'); "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYlAUmyHjBSi"},"outputs":[],"source":["# Combine all Submissions dataframes in one dataframe \n","sdf_master = sdf.append([sdf1], ignore_index = True)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JaD-fPTj5aP"},"outputs":[],"source":["#check number of submissions and comments \n","print('Total number of submissions' + \" \" + str(len(sdf_master)))\n","print('Total number of comments' + \" \" + str(len(cdf_master)))"]},{"cell_type":"markdown","metadata":{"id":"H8nkEbe8qAIb"},"source":["Deal with removed/deleted submissions and comments "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUtVt5HOPrp5"},"outputs":[],"source":["#noticed that some \"removed\" submissions have some text beside the word \"removed\", so this code is to process that.\n","sdf_master['selftext'] = sdf_master['selftext'].replace(r\"\\[removed\\].*\", value='[removed]', regex=True).copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDLIpeTDj8vQ"},"outputs":[],"source":["#Count the number of deleted or removed submissions/comments from the corpus \n","print(len(sdf_master[(sdf_master['selftext'] == '[removed]') | (sdf_master['selftext'] == '[deleted]')] ))\n","print(len(cdf_master[(cdf_master['body'] == '[removed]') | (cdf_master['body'] == '[deleted]')] ))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3veoX_ISlZ7G"},"outputs":[],"source":["#Count the number of deleted or removed submissions which has no comments from the corpus \n","print(len((sdf_master[(((sdf_master['selftext'] == '[removed]') | (sdf_master['selftext'] == '[deleted]')) & (sdf_master['num_comments'] == 0))== True])))\n","\n","#Count the number of deleted or removed comments from the corpus \n","print(len(cdf_master[(cdf_master['body'] == '[removed]') | (cdf_master['body'] == '[deleted]')] ))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otBYdZ9CqYk3"},"outputs":[],"source":["# Remove deleted and removed comments/submissions\n","sdf_master_clean = sdf_master.loc[(((sdf_master['selftext'] == '[removed]') | (sdf_master['selftext'] == '[deleted]')) & (sdf_master['num_comments'] == 0))== False].copy()\n","print('Total number of submissions = ', len(sdf_master_clean))\n","\n","cdf_master_clean = cdf_master.loc[(cdf_master['body'] != '[removed]') & (cdf_master['body'] != '[deleted]')].copy()\n","print('Total number of comments = ', len(cdf_master_clean))\n","#for .copy, check https://www.dataquest.io/blog/settingwithcopywarning/ (I used .loc, but it wasn't enough , .copy solved the issue, it seems it is hidden chain)\n"]},{"cell_type":"markdown","metadata":{"id":"P5MOWV_YrUIe"},"source":["Create documents:  each submission and its comments are considered as one document "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lqq97EIGqA-"},"outputs":[],"source":["#Just add a space before a comment's body to enhnce readability in next steps \n","cdf_master_clean['body'] =  ' ' + cdf_master_clean['body'] "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16Fz80K58cFA"},"outputs":[],"source":["#Group comments related to the same submission\n","cdf_Grouped = cdf_master_clean.groupby('submission_id', as_index=False)['body'].sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-u6tsU409TMB"},"outputs":[],"source":["#check the total no. of unique submissions, and shape of the grouped comments dataframe \n","cdf_Grouped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltqPeMDvqf7T"},"outputs":[],"source":["#check the total no. of submissions in the submissions dataframe \n","# number of submission could be higher due to submissions which have no comments \n","sdf_master_clean.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPEEMq2lD9U1"},"outputs":[],"source":["\n","#Combine submissions with their comments to create documents\n","final_df = sdf_master_clean.merge(cdf_Grouped,  how= 'left',left_on='id', right_on='submission_id')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XX2sex37EJEN"},"outputs":[],"source":["#Check shape and number of documents in the final dataframe \n","final_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BaasEw6nEqwm"},"outputs":[],"source":["#Combine the text of submission and its comments in a single field for future text processing \n","# Note that both submission's title and text are considered becuase there are a lot of submissions having titles without text.\n","final_df['posts_texts'] = final_df[\"title\"].astype(str) + ' ' + final_df[\"selftext\"].astype(str) + ' ' + final_df[\"body\"].astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFNgBU-iFLTg"},"outputs":[],"source":["# Seperate the text for documents\n","posts_texts= final_df['posts_texts'].copy()"]},{"cell_type":"markdown","metadata":{"id":"KRXeuxXt5bed"},"source":["Documents Preprocessing "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2I8aXnr1SyV"},"outputs":[],"source":["#'nan', newline charachter removal, and treatment for VW ID car versions  \n","posts_texts = posts_texts.replace(r'\\\\n',' ', regex=True) # several newlines charachers were noticed after data extraction \n","posts_texts = posts_texts.replace(r'\\b([Nn][Aa][Nn])\\b',' ', regex=True) # 'nan' happended because of combining title of submission and its empty text\n","posts_texts = posts_texts.replace(r'([Hh][Aa]){2,}',' ', regex=True) #  remove hahaha related words\n","posts_texts = posts_texts.replace(r'([lL][Oo][lL]){2,}',' ', regex=True) # remove lollol related words\n","posts_texts = posts_texts.replace(r'(ID4|ID.4)','VWIDIV', regex=True) # to handle Volkswagen ID.4 issue\n","posts_texts = posts_texts.replace(r'(ID3|ID.3)','VWIDIII', regex=True) # to handle Volkswagen ID.3 issue\n","posts_texts = posts_texts.replace(r'(ID2|ID.2)','VWIDII', regex=True) # to handle Volkswagen ID.2 issue\n","posts_texts = posts_texts.replace(r'(ID6|ID.6)','VWIDVI', regex=True) # to handle Volkswagen ID.6 issue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNeMPLaGj9op"},"outputs":[],"source":["#check if we have empty documents \n","len(posts_texts[posts_texts == '']) # answer should be 0 if no empty documents\n","#posts_texts[0:22]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYp7x0yskJIg"},"outputs":[],"source":["#removing links \n","removed_links = posts_texts.replace(r\"[\\[]?[hH][tT][tT][Pp]\\S+\", value='', regex=True).copy()# (\"[([hH]ttp\\S+\", \"\")\n","#note: added [] to the regex, because there was some cells with only the URL put inside [], so removing the http only will lead to having a cell with [], and then empty one after preprocessing \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpBhtuetugXd"},"outputs":[],"source":["# Reseting index\n","removed_links.reset_index(drop= True, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_afC-GD3LrF"},"outputs":[],"source":["#Preparing stopwords based on Gensim set \"STOPWORDS\"\n","my_stop_words = STOPWORDS.union(set(['jpg', 'png', 'yes','isn','aren', 'removed','deleted','www','http', 'https', 'shit', 'lol', 'fuck', 'ev', 've', 'couldn', 'wouldn', 'vehicle', 'car','cars','vehicles' ])) \n","my_stop_words = my_stop_words.difference(set(['system','computer','bill','re'])) \n","print(my_stop_words)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"reQYVfmokY0L"},"outputs":[],"source":["#lower case, tokenize posts including removing punctuations, remove very short words(len=1) and change non-english characters to english ones.\n","# and remove Stop Words\n","def remove_stopwords(texts):\n","    return [[word for word in simple_preprocess(str(doc).encode('utf-8'), deacc=True) if word not in my_stop_words] for doc in texts]\n","\n","data_words_nostops = remove_stopwords(removed_links)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TafTUgrmkce2"},"outputs":[],"source":["# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n","#!python3 -m spacy download en\n","nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lr3EcLxskfDg"},"outputs":[],"source":["def lemmatization(texts, allowed_postags=['NOUN','VERB']): #['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    \"\"\"https://spacy.io/api/annotation\"\"\"\n","    texts_out = []\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) # I understand this is to convert sent from list to str, because nlp takes string\n","        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","        #only allowed post tags are kept and enter lemma\n","    return texts_out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7b0t9OXCkiQe"},"outputs":[],"source":["# Do lemmatization keeping only noun, vb\n","data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN','VERB']) #['NOUN', 'ADJ', 'VERB', 'ADV'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zppvN5DjkyGr"},"outputs":[],"source":["# Create Dictionary\n","id2word = corpora.Dictionary(data_lemmatized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8vSX7OBk77u"},"outputs":[],"source":["# Create Corpus\n","texts = data_lemmatized\n","# Term Document Frequency\n","corpus = [id2word.doc2bow(text) for text in texts]\n","\n","# View\n","print(corpus[-1:])\n","#type(corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eURW2ls5lABg"},"outputs":[],"source":["print('Number of unique tokens: %d' % len(id2word)) #Dictionary\n","print('Number of documents: %d' % len(corpus))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnMOrLs6Vzes"},"outputs":[],"source":["#Set the path to the Mallet binary\n","import os\n","os.environ['MALLET_HOME'] = '/content/mallet-2.0.8'\n","mallet_path = '/content/mallet-2.0.8/bin/mallet' # you should NOT need to change this"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faUcMIXPbY1e"},"outputs":[],"source":["import logging\n","logging.basicConfig(filename='gensim.log',\n","                    format=\"%(asctime)s:%(levelname)s:%(message)s\",\n","                    level=logging.INFO)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBLtOYo3V_x5"},"outputs":[],"source":["ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=39, id2word=id2word, optimize_interval=10, iterations= 2000, random_seed=42 )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRfSEoaulGhD"},"outputs":[],"source":["from pprint import pprint #pretty print"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f06TB8DcxOpq"},"outputs":[],"source":["#Coherence c_v\n","\n","from gensim.models.coherencemodel import CoherenceModel\n","\n","# Compute Coherence Score\n","coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n","coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n","print('\\nCoherence Score: ', coherence_ldamallet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbcVZK2G987e"},"outputs":[],"source":["Topics= ldamallet.show_topics(num_topics=39, num_words=20)       # (formatted=False)\n","#pprint(ldamallet.show_topics(formatted=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKW1Jhf4-Bkk"},"outputs":[],"source":["import csv\n","with open('TopicsGrouped39_2000it.csv','w') as f:\n","     write = csv.writer(f)\n","     write.writerows(Topics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwiZN6JlbTRz"},"outputs":[],"source":["6#Convert the Mallet model to Gensim format.\n","gensimmodelMall = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet) #ldamallet \n","#https://github.com/polsci/colab-gensim-mallet/blob/master/topic-modeling-with-colab-gensim-mallet.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rhuu7AwBlUuw"},"outputs":[],"source":["# Visualize the topics\n","pyLDAvis.enable_notebook()\n","vis = gensimvis.prepare(gensimmodelMall, corpus, id2word, sort_topics=False) #https://github.com/bmabey/pyLDAvis/issues/127\n","vis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"douoiRfzLTYB"},"outputs":[],"source":["pyLDAvis.save_html(vis, 'lda39G.html')"]},{"cell_type":"code","source":["all_topics = gensimmodelMall.get_document_topics(corpus, minimum_probability=0.0)\n","all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n","all_topics_numpy = all_topics_csr.T.toarray()\n","all_topics_df = pd.DataFrame(all_topics_numpy)\n","\n","#https://stackoverflow.com/questions/46574720/python-gensim-lda-add-the-topic-to-the-document-after-getting-the-topics/61397756#61397756\n","#https://radimrehurek.com/gensim/matutils.html#gensim.matutils.corpus2csc"],"metadata":{"id":"PKMP1n76_Op3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","plt.figure(figsize=(50,50))\n","svm = sns.heatmap(all_topics_df.corr(method= 'spearman'), annot=True)\n","figure = svm.get_figure()    \n","figure.savefig('svm_heatmapgrpspear.png', dpi=400)"],"metadata":{"id":"in65_NSatMSQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"LDA_Mallet_Grouped.ipynb","provenance":[{"file_id":"1e3hulM3QTmuXko2YW84JDVH-MJ3nduMu","timestamp":1653246645954},{"file_id":"1GFlJO0WQG2lbwYwtHNxsGh7bP72GHL8h","timestamp":1646856069959},{"file_id":"12Hjxg4ktUg6bWf4xK5YrLbDiY0UTqo8d","timestamp":1640280985422},{"file_id":"1iH9E2VlLoYy0jEHNHyixGQSFTRg78alV","timestamp":1638809486078}],"private_outputs":true,"mount_file_id":"1gjNkCMWIceRA33_bUlyPJmdF2AeH6Qkf","authorship_tag":"ABX9TyPJiXsEiZMOS90iq55nFJFY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}