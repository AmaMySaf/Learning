{"cells":[{"cell_type":"markdown","source":["Copyright 2022 Amani Hamdan\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","       http://www.apache.org/licenses/LICENSE-2.0\n","Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and limitations under the License.\n"],"metadata":{"id":"vGttobTYh_-l"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mT9_r5voxOF","jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[],"source":["!pip install -U spacy "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuJcjxnaC7u5","jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[],"source":["!python -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGN1jrCPi_V-"},"outputs":[],"source":["# Main libraries \n","import pandas as pd\n","import numpy as np\n","\n","# Preprocessing\n","#!pip install gensim # i think it is already there in colab\n","import gensim\n","from gensim.utils import simple_preprocess \n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","#to increase the space of displaying texts\n","pd.options.display.max_colwidth = 200 "]},{"cell_type":"markdown","metadata":{"id":"_dQO01r4qpcb"},"source":["Load extracted submissions and comments "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMEXwD-LiJjU","tags":[]},"outputs":[],"source":["# use this as many Comments files you have, just change the dataframe number and file name \n","cdf= pd.read_csv('comments_ve2y.csv', encoding = \"utf-8\", on_bad_lines='skip') \n","cdf1= pd.read_csv('comments_ca1y.csv', encoding = \"utf-8\", on_bad_lines='skip') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dwn_DxRaih_t"},"outputs":[],"source":["cdf_ve = cdf[cdf['created_utc']> 1606780799].copy() # selecting comments after 1/12/2020 [as I have some older data in my file]\n","print(cdf_ve.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ru-haypki8fu"},"outputs":[],"source":["# Combine all Comments dataframes in one dataframe \n","cdf_master = cdf_ve.append([cdf1], ignore_index= True)  #ignore_index= True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QRgW3OGjtT5"},"outputs":[],"source":["# use this as many Submissions files you have, just change the dataframe number and file name \n","sdf= pd.read_csv('submissions_ve1y.csv', encoding = \"utf-8\", on_bad_lines='skip'); \n","sdf1= pd.read_csv('submissions_ca1y.csv', encoding = \"utf-8\", on_bad_lines='skip'); "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYlAUmyHjBSi"},"outputs":[],"source":["# Combine all Submissions dataframes in one dataframe \n","sdf_master = sdf.append([sdf1], ignore_index = True)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JaD-fPTj5aP"},"outputs":[],"source":["#check number of submissions and comments \n","print('Total number of submissions' + \" \" + str(len(sdf_master)))\n","print('Total number of comments' + \" \" + str(len(cdf_master)))"]},{"cell_type":"markdown","metadata":{"id":"H8nkEbe8qAIb"},"source":["Deal with removed/deleted submissions and comments "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUtVt5HOPrp5"},"outputs":[],"source":["#noticed that some \"removed\" submissions have some text beside the word \"removed\", so this code is to process that.\n","sdf_master['selftext'] = sdf_master['selftext'].replace(r\"\\[removed\\].*\", value='[removed]', regex=True).copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDLIpeTDj8vQ"},"outputs":[],"source":["#Count the number of deleted or removed submissions/comments from the corpus \n","print(len(sdf_master[(sdf_master['selftext'] == '[removed]') | (sdf_master['selftext'] == '[deleted]')] ))\n","print(len(cdf_master[(cdf_master['body'] == '[removed]') | (cdf_master['body'] == '[deleted]')] ))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3veoX_ISlZ7G"},"outputs":[],"source":["#Count the number of deleted or removed submissions which has no comments from the corpus \n","print(len((sdf_master[(((sdf_master['selftext'] == '[removed]') | (sdf_master['selftext'] == '[deleted]')) & (sdf_master['num_comments'] == 0))== True])))\n","\n","#Count the number of deleted or removed comments from the corpus \n","print(len(cdf_master[(cdf_master['body'] == '[removed]') | (cdf_master['body'] == '[deleted]')] ))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otBYdZ9CqYk3"},"outputs":[],"source":["# Remove deleted and removed comments/submissions\n","sdf_master_clean = sdf_master.loc[(((sdf_master['selftext'] == '[removed]') | (sdf_master['selftext'] == '[deleted]')) & (sdf_master['num_comments'] == 0))== False].copy()\n","print('Total number of submissions = ', len(sdf_master_clean))\n","\n","cdf_master_clean = cdf_master.loc[(cdf_master['body'] != '[removed]') & (cdf_master['body'] != '[deleted]')].copy()\n","print('Total number of comments = ', len(cdf_master_clean))\n","#for .copy, check https://www.dataquest.io/blog/settingwithcopywarning/ (I used .loc, but it wasn't enough , .copy solved the issue, it seems it is hidden chain)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7W8KEQQX2vFp"},"outputs":[],"source":["sdf_master_clean[\"selftext\"].replace(r'\\[removed\\]', \" \", regex=True, inplace=True)\n","sdf_master_clean[\"selftext\"].replace(r'\\[deleted\\].*', \" \", regex=True, inplace=True)\n","#second answer : https://stackoverflow.com/questions/37593550/replace-method-not-working-on-pandas-dataframe"]},{"cell_type":"markdown","metadata":{"id":"abEGLRiOMeqB"},"source":["Append Submissions and Comments  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BaasEw6nEqwm"},"outputs":[],"source":["# Note that both submission's title and text are considered becuase there are a lot of submissions having titles without text.\n","sdf_master_clean['combined_text'] = sdf_master_clean[\"title\"].astype(str) + \". \" + sdf_master_clean[\"selftext\"].astype(str)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFNgBU-iFLTg"},"outputs":[],"source":["#Extract the text of comments/submissions and combine them, calling them posts this point forward\n","posts_texts = sdf_master_clean['combined_text'].append(cdf_master_clean['body'])\n","print('Total number of posts = ', len(posts_texts))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCjp4tJoRcNj"},"outputs":[],"source":["#to handle emoji encoding, as vader recognizes emojies \n","posts_texts = posts_texts.str.encode('latin', errors= 'ignore').str.decode('utf-8', errors= 'ignore')"]},{"cell_type":"markdown","metadata":{"id":"KRXeuxXt5bed"},"source":["Documents' test Preprocessing "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2I8aXnr1SyV"},"outputs":[],"source":["#'nan' and newline charachter removal \n","\n","posts_texts = posts_texts.replace(r'\\.+', \".\", regex=True) # replace multiple dots with single fullstop for sentence tokenization for similarity analysis \n","posts_texts = posts_texts.replace(r'\\\\n',' ', regex=True) # several newlines charachers were noticed after data extraction \n","posts_texts = posts_texts.replace(r'\\b([Nn][Aa][Nn])\\b',' ', regex=True) # 'nan' happended because of combining title of submission and its empty text\n","posts_texts = posts_texts.replace(r'([Hh][Aa]){2,}',' ', regex=True) #  remove hahaha related words\n","posts_texts = posts_texts.replace(r'([lL][Oo][lL]){2,}',' ', regex=True) # remove lollol related words\n","posts_texts = posts_texts.replace(r'(ID4|ID.4)','VWIDIV', regex=True) # to handle Volkswagen ID.4 issue\n","posts_texts = posts_texts.replace(r'(ID3|ID.3)','VWIDIII', regex=True) # to handle Volkswagen ID.3 issue\n","posts_texts = posts_texts.replace(r'(ID2|ID.2)','VWIDII', regex=True) # to handle Volkswagen ID.2 issue\n","posts_texts = posts_texts.replace(r'(ID6|ID.6)','VWIDVI', regex=True) # to handle Volkswagen ID.6 issue\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNeMPLaGj9op"},"outputs":[],"source":["#check if we have empty documents \n","print(len(posts_texts[posts_texts == ''])) # answer should be 0 if no empty documents\n","posts_texts[14:16]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYp7x0yskJIg"},"outputs":[],"source":["#removing links \n","removed_links = posts_texts.replace(r\"[\\[]?[hH][tT][tT][Pp]\\S+\", value='', regex=True).copy()# (\"[([hH]ttp\\S+\", \"\")\n","#note: added [] to the regex, because there was some cells with only the URL put inside [], so removing the http only will lead to having a cell with [], and then empty one after preprocessing \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uD32fw4ORcNn"},"outputs":[],"source":["print(len(removed_links[removed_links == '']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmORaqZ3RcNo"},"outputs":[],"source":["removed_links.replace('', np.nan, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_c2rc7IRcNo"},"outputs":[],"source":["removed_links.shape[0] - removed_links.dropna().shape[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ta1UVxr9RcNp"},"outputs":[],"source":["removed_links.dropna(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGeRFXv1RcNr"},"outputs":[],"source":["print(len(removed_links[removed_links == '']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpBhtuetugXd"},"outputs":[],"source":["# Reseting index\n","removed_links.reset_index(drop= True, inplace=True)"]},{"cell_type":"markdown","source":["Calculate word2vec"],"metadata":{"id":"4t5a-kEoZf-x"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6e6ahm31RcN4"},"outputs":[],"source":["#import selected aspects \n","topics_words_df= pd.read_csv('aspects17.csv', on_bad_lines='skip') \n","topics_words_df.columns =('TopicNo', 'TopicName', 'TopicCategory', 'TopicKeyWords')\n","topics_words_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMaFkLwyv7Kp"},"outputs":[],"source":["nlp2 = spacy.load('en_core_web_lg')\n","#Define function to get word2vec for a sentence \n","def get_vec(x):\n","  doc = nlp2(x)\n","  vec = doc.vector\n","  return vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q78vXH97KBlI"},"outputs":[],"source":["#calculate vec for aspects keywords \n","topics_words_df['key_words_vec'] = topics_words_df['TopicKeyWords'].apply(lambda x: get_vec(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAKciBso8q0y"},"outputs":[],"source":["#Prepare posts for word2vec-similarity calculation \n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0ihZChc1KxK"},"outputs":[],"source":["#tokenize sentences \n","posts_sent = removed_links.apply(lambda x : sent_tokenize(x)).apply(pd.Series,1).stack()\n","Post_sent_df = pd.DataFrame(posts_sent)#change serise to dataframe \n","Post_sent_df.columns = ['Sent_text']\n","Post_sent_df.shape\n","Post_sent_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zLFCmA62jbBa"},"outputs":[],"source":["#remove deaccent\n","from gensim.utils import deaccent\n","def clean_text (text):\n","    text = deaccent(text)\n","    return text\n","\n","Post_sent_df['Sent_text'].apply(lambda x : clean_text(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GiuMiEfCRcN9"},"outputs":[],"source":["#calculate vec for sentences \n","Post_sent_df['Sent_vec'] = Post_sent_df['Sent_text'].apply(lambda x: get_vec(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRiQHztrRcN-"},"outputs":[],"source":["#calculate similarity values\n","for index, row in Post_sent_df.iterrows():\n","    row = Post_sent_df['Sent_vec'][index]\n","    for index_tp, row_tp in topics_words_df.iterrows():\n","        row_tp = topics_words_df['key_words_vec'][index_tp]\n","        cosine_similarity = np.dot(row, row_tp)/(np.linalg.norm(row)* np.linalg.norm(row_tp))\n","    #    new_col.append(sim_value)\n","        col_name = str(index_tp)\n","        Post_sent_df.at[index , index_tp] = cosine_similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7P1KqHzJRcN_"},"outputs":[],"source":["#delete sentences vector column as it causes large size/ memory crash in further processing \n","Post_sent_df.drop(['Sent_vec'], axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBZLVuaIPdud"},"outputs":[],"source":["#to check columns \n","Post_sent_df.iloc[:, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CMFSvH1v2CyG"},"outputs":[],"source":["#Find the most similar aspect (the aspect associated with each sentence )\n","Post_sent_df['Topic'] = Post_sent_df.values[:,1:18].argmax(1) # as similarity columns starts from column index = 1\n","#https://stackoverflow.com/questions/43330555/pandas-python-max-in-columns-define-new-value-in-new-column\n","\n","#second tried case, #to ignore if similarity is less tha 0.5 \n","Post_sent_df['Topic'] = (\n","   Post_sent_df.iloc[:, 1:18].mask(~Post_sent_df.iloc[:, 1:18].ge(0.65).any(1), other=pd.NA)\n","           .idxmax(1).fillna('none'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9FdL0p6vRcOH"},"outputs":[],"source":["len(Post_sent_df['Topic'].value_counts()) # check value "]},{"cell_type":"markdown","source":["Sentiment Analysis , \n","Thanks to https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664\n"],"metadata":{"id":"plsLVIy0dkUu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGK1jbzEO3Zi"},"outputs":[],"source":["import nltk\n","nltk.download('vader_lexicon')\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","sid = SentimentIntensityAnalyzer()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_aBg85gKe2I"},"outputs":[],"source":["#sentiment calculation \n","Post_sent_df['scores'] = Post_sent_df['Sent_text'].apply(lambda T: sid.polarity_scores(T))\n","Post_sent_df['compoundSco'] = Post_sent_df['scores'].apply(lambda score_dict: score_dict['compound'])\n","Post_sent_df['comp_score'] = Post_sent_df['compoundSco'].apply(lambda c: 'pos' if c >0 else 'neu' if c==0 else 'neg')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xtvJYUXPPku"},"outputs":[],"source":["Post_sent_df['comp_score'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rolvTdY8RcOM"},"outputs":[],"source":["Post_sent_df['comp_score'].value_counts().plot(kind = 'bar', title = 'Count of sentences per sentiment type')"]},{"cell_type":"code","source":["# to sum sentiment per dominant topic \n","Post_sent_df_grp_sen = Post_sent_df.groupby('Topic', as_index=False)['compoundSco'].mean()"],"metadata":{"id":"EYdkxidTeCdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnxbdo8-hPXc"},"outputs":[],"source":["Post_sent_df_grp_sen.plot(x='Topic', y='compoundSco', kind = 'bar')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ABSA.ipynb","provenance":[{"file_id":"1as81cqkqKftHe8SOO1cCdnwUpjdtFzMu","timestamp":1646923222925},{"file_id":"1y4H3_PKVEkDgs5P3bqUXY8xCiGxL-TCa","timestamp":1644850198786},{"file_id":"1-9zV5pSVkXOOxvA5cLdgt8CFNsWR6Hj6","timestamp":1643750086926},{"file_id":"1GFlJO0WQG2lbwYwtHNxsGh7bP72GHL8h","timestamp":1643396926087},{"file_id":"12Hjxg4ktUg6bWf4xK5YrLbDiY0UTqo8d","timestamp":1640280985422},{"file_id":"1iH9E2VlLoYy0jEHNHyixGQSFTRg78alV","timestamp":1638809486078}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}